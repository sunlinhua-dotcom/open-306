#!/usr/bin/env python3
"""
æµè§ˆå™¨æŠ“å–å·¥å…· â€” ç›´æ¥è®¿é—®å¤§ä¼—ç‚¹è¯„/å°çº¢ä¹¦/çŸ¥ä¹ç­‰å¹³å°
ä½¿ç”¨ Playwright + ç”¨æˆ· Chrome profileï¼ˆå¤ç”¨å·²ç™»å½•çŠ¶æ€ï¼‰

ç”¨æ³•:
  python3 browser_fetch.py --search "å¤–æ»©é¤å…" --site dianping
  python3 browser_fetch.py "https://www.xiaohongshu.com/explore/xxx"
  python3 browser_fetch.py --login dianping

ç¯å¢ƒå˜é‡:
  PLAYWRIGHT_BROWSERS_PATH  â€” Playwright æµè§ˆå™¨è·¯å¾„
  CHROME_USER_DATA_DIR      â€” Chrome ç”¨æˆ·æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰
"""

import argparse
import json
import os
import sys
import time
import re
import tempfile
import shutil
import urllib.parse

# ========== è·¯å¾„é…ç½® ==========
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OPENCLAW_ROOT = os.environ.get("OPENCLAW_HOME", os.path.dirname(os.path.dirname(os.path.dirname(SCRIPT_DIR))))

if "PLAYWRIGHT_BROWSERS_PATH" not in os.environ:
    os.environ["PLAYWRIGHT_BROWSERS_PATH"] = os.path.join(OPENCLAW_ROOT, ".venv", "browsers")

# Cookie æŒä¹…åŒ–ç›®å½•
COOKIE_DIR = os.path.join(OPENCLAW_ROOT, ".openclaw", "cookies")
os.makedirs(COOKIE_DIR, exist_ok=True)

# Chrome Debug ç«¯å£
CDP_PORT = int(os.environ.get("CDP_PORT", "9222"))

# Playwright å»¶è¿Ÿå¯¼å…¥
sync_playwright = None

def _ensure_playwright():
    global sync_playwright
    if sync_playwright is None:
        from playwright.sync_api import sync_playwright as sp
        sync_playwright = sp


def _check_cdp_port(port=None):
    """æ£€æŸ¥ Chrome Debug ç«¯å£æ˜¯å¦å¯ç”¨"""
    port = port or CDP_PORT
    try:
        import urllib.request as ur
        req = ur.Request(f"http://127.0.0.1:{port}/json/version")
        with ur.urlopen(req, timeout=2) as resp:
            data = json.loads(resp.read())
            return data
    except Exception:
        return None


# ========== å¹³å°é…ç½® ==========

PLATFORMS = {
    "dianping": {
        "name": "å¤§ä¼—ç‚¹è¯„",
        "domain": "dianping.com",
        "search_url": "https://www.dianping.com/search/keyword/{city_id}/0_{query}",
        "home_url": "https://www.dianping.com",
        "default_city": "1",  # ä¸Šæµ·
        "cities": {"ä¸Šæµ·": "1", "åŒ—äº¬": "2", "å¹¿å·": "4", "æ·±åœ³": "7", "æ­å·": "5", "å—äº¬": "9", "æˆéƒ½": "8", "æ­¦æ±‰": "13", "è¥¿å®‰": "17", "é‡åº†": "30"},
    },
    "xiaohongshu": {
        "name": "å°çº¢ä¹¦",
        "domain": "xiaohongshu.com",
        "search_url": "https://www.xiaohongshu.com/search_result?keyword={query}&source=web_search_result_note",
        "home_url": "https://www.xiaohongshu.com",
    },
    "xhs": {  # å°çº¢ä¹¦åˆ«å
        "name": "å°çº¢ä¹¦",
        "domain": "xiaohongshu.com",
        "search_url": "https://www.xiaohongshu.com/search_result?keyword={query}&source=web_search_result_note",
        "home_url": "https://www.xiaohongshu.com",
    },
    "zhihu": {
        "name": "çŸ¥ä¹",
        "domain": "zhihu.com",
        "search_url": "https://www.zhihu.com/search?type=content&q={query}",
        "home_url": "https://www.zhihu.com",
    },
    "weibo": {
        "name": "å¾®åš",
        "domain": "weibo.com",
        "search_url": "https://s.weibo.com/weibo?q={query}",
        "home_url": "https://weibo.com",
    },
    "bilibili": {
        "name": "Bç«™",
        "domain": "bilibili.com",
        "search_url": "https://search.bilibili.com/all?keyword={query}",
        "home_url": "https://www.bilibili.com",
    },
}


# ========== Cookie ç®¡ç† ==========

def get_cookie_path(platform):
    return os.path.join(COOKIE_DIR, f"{platform}_cookies.json")

def save_cookies(context, platform):
    """ä¿å­˜æµè§ˆå™¨ Cookie"""
    cookies = context.cookies()
    path = get_cookie_path(platform)
    with open(path, "w") as f:
        json.dump(cookies, f, ensure_ascii=False, indent=2)
    print(f"âœ… Cookie å·²ä¿å­˜åˆ° {path} ({len(cookies)} æ¡)", file=sys.stderr)

def load_cookies(context, platform):
    """åŠ è½½å·²ä¿å­˜çš„ Cookie"""
    path = get_cookie_path(platform)
    if os.path.exists(path):
        with open(path) as f:
            cookies = json.load(f)
        context.add_cookies(cookies)
        print(f"ğŸª å·²åŠ è½½ {platform} Cookie ({len(cookies)} æ¡)", file=sys.stderr)
        return True
    return False


# ========== æµè§ˆå™¨å¯åŠ¨ ==========

def create_browser_context(playwright, headless=True, mobile=False, use_cdp=False):
    """åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒ CDP è¿æ¥å’Œç‹¬ç«‹å¯åŠ¨ä¸¤ç§æ¨¡å¼"""
    
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨ï¼ˆmacOS æƒé™é—®é¢˜ï¼‰
    pw_tmp = os.path.join(OPENCLAW_ROOT, ".openclaw", "pw_tmp")
    os.makedirs(pw_tmp, exist_ok=True)
    os.environ["TMPDIR"] = pw_tmp
    
    # æ¨¡å¼1: CDP è¿æ¥å·²æœ‰ Chrome Debug å®ä¾‹ï¼ˆæ¨èï¼‰
    if use_cdp:
        cdp_info = _check_cdp_port()
        if cdp_info:
            print(f"ğŸ”— CDP è¿æ¥: Chrome {cdp_info.get('Browser', '')}", file=sys.stderr)
            browser = playwright.chromium.connect_over_cdp(f"http://127.0.0.1:{CDP_PORT}")
            # CDP æ¨¡å¼ä¸‹ä½¿ç”¨å·²æœ‰çš„é»˜è®¤ä¸Šä¸‹æ–‡
            if browser.contexts:
                ctx = browser.contexts[0]
                print(f"âœ… å·²è¿æ¥ï¼ˆå¤ç”¨å·²ç™»å½•ä¼šè¯ï¼‰", file=sys.stderr)
                return browser, ctx
            else:
                ctx = browser.new_context(locale="zh-CN", timezone_id="Asia/Shanghai")
                return browser, ctx
        else:
            print(f"âš ï¸ CDP ç«¯å£ {CDP_PORT} ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç‹¬ç«‹ Chromium", file=sys.stderr)
    
    # æ¨¡å¼2: ç‹¬ç«‹ Playwright Chromiumï¼ˆheadlessï¼‰
    launch_args = [
        '--no-sandbox',
        '--disable-blink-features=AutomationControlled',
        '--disable-dev-shm-usage',
    ]
    
    browser = playwright.chromium.launch(
        headless=headless,
        args=launch_args,
    )
    
    context_opts = {
        "viewport": {"width": 1920, "height": 1080},
        "locale": "zh-CN",
        "timezone_id": "Asia/Shanghai",
        "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    }
    
    if mobile:
        context_opts.update({
            "viewport": {"width": 390, "height": 844},
            "user_agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 18_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0 Mobile/15E148 Safari/604.1",
            "is_mobile": True,
        })
    
    ctx = browser.new_context(**context_opts)
    
    # åæ£€æµ‹è„šæœ¬
    ctx.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });
        Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
        window.chrome = { runtime: {} };
    """)
    
    return browser, ctx


# ========== ç™»å½•æµç¨‹ ==========

def login_platform(platform):
    """é headless æµè§ˆå™¨ç™»å½•ï¼Œä¿å­˜ Cookie"""
    config = PLATFORMS.get(platform)
    if not config:
        print(f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
        sys.exit(1)
    
    _ensure_playwright()
    
    print(f"\nğŸ” æ­£åœ¨æ‰“å¼€ {config['name']}ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•...", file=sys.stderr)
    print("   ç™»å½•å®Œæˆåï¼Œå…³é—­æµè§ˆå™¨çª—å£å³å¯ã€‚\n", file=sys.stderr)
    
    with sync_playwright() as p:
        browser, ctx = create_browser_context(p, headless=False)
        page = ctx.new_page()
        
        try:
            page.goto(config["home_url"], wait_until="domcontentloaded", timeout=30000)
            
            # ç­‰å¾…ç”¨æˆ·å…³é—­æµè§ˆå™¨æˆ–æŒ‰ Ctrl+C
            print("â³ ç­‰å¾…ç™»å½•... ç™»å½•å®Œæˆåå…³é—­æµè§ˆå™¨çª—å£", file=sys.stderr)
            while True:
                try:
                    time.sleep(2)
                    page.title()  # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜å­˜åœ¨
                except:
                    break
                    
        except KeyboardInterrupt:
            print("\nâ¹ åœæ­¢ç­‰å¾…", file=sys.stderr)
        
        # ä¿å­˜ Cookie
        save_cookies(ctx, platform)
        browser.close()


# ========== æå–å™¨ ==========

def extract_dianping_search(page):
    """å¤§ä¼—ç‚¹è¯„æœç´¢ç»“æœæå–"""
    results = []
    time.sleep(2)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
    title = page.title()
    if "éªŒè¯" in title:
        print("âš ï¸ å¤§ä¼—ç‚¹è¯„è¦æ±‚éªŒè¯ï¼Œè¯·å…ˆç™»å½•: python3 browser_fetch.py --login dianping", file=sys.stderr)
        return results
    
    # æœç´¢ç»“æœåˆ—è¡¨
    items = page.query_selector_all('[class*="shopInfo"], [class*="shop-info"], .shop-list li, [data-shopid]')
    
    if not items:
        # å°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
        items = page.query_selector_all('#shop-all-list li, .shop-list ul li')
    
    for item in items[:10]:
        try:
            name_el = item.query_selector('h4, .tit, [class*="shopName"], [class*="shop-name"], a.name')
            star_el = item.query_selector('[class*="star"], [class*="rating"]')
            price_el = item.query_selector('[class*="mean-price"], [class*="price"]')
            tag_el = item.query_selector('[class*="tag"], [class*="category"]')
            addr_el = item.query_selector('[class*="addr"], [class*="address"], [class*="region"]')
            link_el = item.query_selector('a[href*="shop"]')
            
            name = name_el.inner_text().strip() if name_el else ""
            if not name:
                continue
                
            results.append({
                "name": name,
                "rating": star_el.inner_text().strip() if star_el else "",
                "price": price_el.inner_text().strip() if price_el else "",
                "category": tag_el.inner_text().strip() if tag_el else "",
                "address": addr_el.inner_text().strip() if addr_el else "",
                "url": link_el.get_attribute("href") if link_el else "",
            })
        except:
            continue
    
    return results


def extract_dianping_shop(page):
    """å¤§ä¼—ç‚¹è¯„åº—é“ºè¯¦æƒ…æå–"""
    time.sleep(2)
    info = {}
    
    # åº—å
    name_el = page.query_selector('h1, .shop-name, [class*="shopName"]')
    info["name"] = name_el.inner_text().strip() if name_el else ""
    
    # è¯„åˆ†
    star_el = page.query_selector('[class*="star"], [class*="score"]')
    info["rating"] = star_el.inner_text().strip() if star_el else ""
    
    # äººå‡
    price_el = page.query_selector('[class*="avgPrice"], [class*="price"]')
    info["avg_price"] = price_el.inner_text().strip() if price_el else ""
    
    # åœ°å€
    addr_el = page.query_selector('[class*="address"], [itemprop="street-address"]')
    info["address"] = addr_el.inner_text().strip() if addr_el else ""
    
    # è¯„è®ºæ‘˜è¦
    comments = []
    comment_els = page.query_selector_all('[class*="comment-item"], [class*="review"]')
    for c in comment_els[:5]:
        text = c.inner_text().strip()[:200]
        if text:
            comments.append(text)
    info["comments"] = comments
    
    # èœå“æ¨è
    dishes = []
    dish_els = page.query_selector_all('[class*="recommend-dish"], [class*="rec-tag"]')
    for d in dish_els[:10]:
        dishes.append(d.inner_text().strip())
    info["recommended_dishes"] = dishes
    
    return info


def extract_xiaohongshu_search(page):
    """å°çº¢ä¹¦æœç´¢ç»“æœæå–"""
    results = []
    time.sleep(3)
    
    title = page.title()
    if "ç™»å½•" in title or "éªŒè¯" in title:
        print("âš ï¸ å°çº¢ä¹¦è¦æ±‚ç™»å½•ï¼Œè¯·å…ˆç™»å½•: python3 browser_fetch.py --login xiaohongshu", file=sys.stderr)
        return results
    
    # æœç´¢ç»“æœå¡ç‰‡
    items = page.query_selector_all('[class*="note-item"], [class*="search-note"], section[class*="note"]')
    
    if not items:
        items = page.query_selector_all('a[href*="/explore/"]')
    
    for item in items[:10]:
        try:
            title_el = item.query_selector('[class*="title"], [class*="desc"], span')
            author_el = item.query_selector('[class*="author"], [class*="name"]')
            like_el = item.query_selector('[class*="like"], [class*="count"]')
            link_el = item.query_selector('a[href*="explore"]') or item
            
            note_title = title_el.inner_text().strip() if title_el else ""
            if not note_title:
                note_title = item.inner_text().strip()[:100]
            
            href = ""
            try:
                href = link_el.get_attribute("href") or ""
                if href and not href.startswith("http"):
                    href = "https://www.xiaohongshu.com" + href
            except:
                pass
            
            results.append({
                "title": note_title,
                "author": author_el.inner_text().strip() if author_el else "",
                "likes": like_el.inner_text().strip() if like_el else "",
                "url": href,
            })
        except:
            continue
    
    return results


def extract_xiaohongshu_note(page):
    """å°çº¢ä¹¦ç¬”è®°è¯¦æƒ…æå–"""
    time.sleep(3)
    info = {}
    
    title_el = page.query_selector('[class*="title"], h1')
    info["title"] = title_el.inner_text().strip() if title_el else ""
    
    # æ­£æ–‡
    content_el = page.query_selector('[class*="note-text"], [class*="content"], #detail-desc')
    info["content"] = content_el.inner_text().strip()[:2000] if content_el else ""
    
    # ä½œè€…
    author_el = page.query_selector('[class*="author-name"], [class*="username"]')
    info["author"] = author_el.inner_text().strip() if author_el else ""
    
    # ç‚¹èµ/æ”¶è—
    like_el = page.query_selector('[class*="like-count"], [class*="like"]')
    info["likes"] = like_el.inner_text().strip() if like_el else ""
    
    # è¯„è®º
    comments = []
    comment_els = page.query_selector_all('[class*="comment-item"], [class*="comment-text"]')
    for c in comment_els[:5]:
        comments.append(c.inner_text().strip()[:200])
    info["comments"] = comments
    
    return info


def extract_generic(page):
    """é€šç”¨ç½‘é¡µæå–"""
    time.sleep(2)
    
    # æå–ä¸»è¦æ–‡æœ¬å†…å®¹
    for sel in ['article', 'main', '.content', '#content', '.article', '.post']:
        el = page.query_selector(sel)
        if el:
            return {"content": el.inner_text().strip()[:5000], "url": page.url}
    
    # å…œåº•ï¼šæå– body
    body = page.query_selector('body')
    text = body.inner_text().strip()[:5000] if body else ""
    return {"content": text, "url": page.url}


# ========== å¹³å°æ£€æµ‹ ==========

def detect_platform(url):
    """æ ¹æ® URL æ£€æµ‹å¹³å°"""
    for key, config in PLATFORMS.items():
        if config["domain"] in url:
            return key
    return None


# ========== æœç´¢ ==========

def search_platform(query, platform, num=5, context=None, city=None):
    """åœ¨æŒ‡å®šå¹³å°å†…æœç´¢ï¼Œç™»å½•å¤±è´¥è‡ªåŠ¨åˆ‡æ¢åˆ°æœç´¢å¼•æ“æ–¹æ¡ˆ"""
    config = PLATFORMS.get(platform)
    if not config:
        print(f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}", file=sys.stderr)
        return []
    
    search_url = config["search_url"]
    
    # å¤§ä¼—ç‚¹è¯„éœ€è¦åŸå¸‚ ID
    if platform == "dianping":
        city_id = config["default_city"]
        if city and city in config.get("cities", {}):
            city_id = config["cities"][city]
        search_url = search_url.format(city_id=city_id, query=urllib.parse.quote(query))
    else:
        search_url = search_url.format(query=urllib.parse.quote(query))
    
    print(f"ğŸ” åœ¨ {config['name']} æœç´¢: {query}", file=sys.stderr)
    print(f"   URL: {search_url}", file=sys.stderr)
    
    page = context.new_page()
    results = []
    need_fallback = False
    
    try:
        page.goto(search_url, wait_until="domcontentloaded", timeout=20000)
        time.sleep(3)
        
        # æ£€æµ‹ç™»å½•é‡å®šå‘
        current_url = page.url
        page_text = page.inner_text("body")[:500]
        if ("login" in current_url or "pclogin" in current_url 
            or "ç™»å½•åæŸ¥çœ‹" in page_text or "è¯·ç™»å½•" in page_text[:100]
            or "æ‰«ç " in page_text[:200] and "æœç´¢" not in page.title()):
            print(f"âš ï¸ éœ€è¦ç™»å½•ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æœç´¢å¼•æ“æ–¹æ¡ˆ", file=sys.stderr)
            need_fallback = True
        else:
            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
            page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
            time.sleep(1)
            
            if platform in ("dianping",):
                results = extract_dianping_search(page)
            elif platform in ("xiaohongshu", "xhs"):
                results = extract_xiaohongshu_search(page)
            else:
                text = page.inner_text("body")[:3000]
                results = [{"content": text, "url": search_url}]
        
    except Exception as e:
        print(f"âš ï¸ ç›´æ¥æœç´¢å¤±è´¥: {e}", file=sys.stderr)
        need_fallback = True
    finally:
        page.close()
    
    # æ— ç»“æœæˆ–éœ€è¦ç™»å½•æ—¶ï¼Œå›é€€åˆ°æœç´¢å¼•æ“
    if need_fallback or not results:
        print(f"ğŸ”„ ä½¿ç”¨æœç´¢å¼•æ“é—´æ¥æœç´¢ (site:{config['domain']})", file=sys.stderr)
        results = search_via_engine(query, config["domain"], num, context, city)
    
    return results[:num]


def search_via_engine(query, site_domain, num=5, context=None, city=None):
    """é€šè¿‡æœç´¢å¼•æ“é—´æ¥æœç´¢å¹³å°å†…å®¹ï¼ˆä¸éœ€è¦ç™»å½•ï¼‰"""
    # æ„å»ºæœç´¢å¼•æ“æŸ¥è¯¢
    search_query = f"site:{site_domain} {query}"
    if city:
        search_query += f" {city}"
    
    encoded_q = urllib.parse.quote(search_query)
    
    # ä¼˜å…ˆ DuckDuckGoï¼ˆæ—  CAPTCHAï¼‰ï¼Œç„¶å Google
    engines = [
        ("DuckDuckGo", f"https://duckduckgo.com/?q={encoded_q}"),
        ("Google", f"https://www.google.com/search?q={encoded_q}"),
    ]
    
    for engine_name, engine_url in engines:
        print(f"   ğŸŒ {engine_name}: {search_query}", file=sys.stderr)
        
        page = context.new_page()
        try:
            page.goto(engine_url, wait_until="domcontentloaded", timeout=15000)
            time.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
            time.sleep(1)
            
            results = _extract_search_engine_results(page, engine_name, site_domain)
            
            if results:
                print(f"   âœ… {engine_name} è¿”å› {len(results)} æ¡ç»“æœ", file=sys.stderr)
                page.close()
                return results[:num]
            
        except Exception as e:
            print(f"   âš ï¸ {engine_name} å¤±è´¥: {e}", file=sys.stderr)
        finally:
            page.close()
    
    return []


def _extract_search_engine_results(page, engine, site_domain):
    """ä»æœç´¢å¼•æ“ç»“æœé¡µæå–æ•°æ®"""
    results = []
    
    try:
        if engine == "DuckDuckGo":
            # DuckDuckGo ç»“æœæå–
            items = page.query_selector_all("article[data-testid='result']")
            if not items:
                items = page.query_selector_all(".result, .results .result__body, .nrn-react-div")
            
            for item in items:
                try:
                    # æ ‡é¢˜
                    title_el = item.query_selector("h2 a, a[data-testid='result-title-a']")
                    title = title_el.inner_text().strip() if title_el else ""
                    
                    # URL
                    href = title_el.get_attribute("href") if title_el else ""
                    
                    # æ‘˜è¦
                    snippet_el = item.query_selector("span[data-testid='result-snippet'], .result__snippet")
                    snippet = snippet_el.inner_text().strip() if snippet_el else ""
                    
                    if title and site_domain in (href or ""):
                        results.append({
                            "title": title,
                            "url": href,
                            "snippet": snippet,
                            "source": "search_engine",
                        })
                except:
                    continue
        
        elif engine == "Google":
            # Google ç»“æœæå–
            items = page.query_selector_all("div.g, div[data-sokoban-container]")
            
            for item in items:
                try:
                    title_el = item.query_selector("h3")
                    title = title_el.inner_text().strip() if title_el else ""
                    
                    link_el = item.query_selector("a[href]")
                    href = link_el.get_attribute("href") if link_el else ""
                    
                    snippet_el = item.query_selector("div[data-sncf], .VwiC3b, span.st")
                    snippet = snippet_el.inner_text().strip() if snippet_el else ""
                    
                    if title and site_domain in (href or ""):
                        results.append({
                            "title": title,
                            "url": href,
                            "snippet": snippet,
                            "source": "search_engine",
                        })
                except:
                    continue
        
        # é€šç”¨ fallback â€” çº¯æ–‡æœ¬æå–
        if not results:
            body_text = page.inner_text("body")
            # æå–æ‰€æœ‰åŒ…å«ç›®æ ‡åŸŸåçš„é“¾æ¥
            links = page.query_selector_all(f"a[href*='{site_domain}']")
            for link in links[:20]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute("href")
                    if title and len(title) > 3 and href:
                        results.append({
                            "title": title,
                            "url": href,
                            "snippet": "",
                            "source": "search_engine",
                        })
                except:
                    continue
    
    except Exception as e:
        print(f"   âš ï¸ æå–æœç´¢ç»“æœå¤±è´¥: {e}", file=sys.stderr)
    
    return results


# ========== æŠ“å– URL ==========

def fetch_url(url, context):
    """ç”¨æµè§ˆå™¨æŠ“å–æŒ‡å®š URL"""
    platform = detect_platform(url)
    print(f"ğŸ“– æŠ“å–: {url}", file=sys.stderr)
    
    page = context.new_page()
    result = {}
    
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=20000)
        time.sleep(2)
        
        result["url"] = url
        result["title"] = page.title()
        
        if platform == "dianping":
            result["data"] = extract_dianping_shop(page)
        elif platform in ("xiaohongshu", "xhs"):
            result["data"] = extract_xiaohongshu_note(page)
        else:
            result["data"] = extract_generic(page)
            
    except Exception as e:
        result["error"] = str(e)
        print(f"âš ï¸ æŠ“å–å¤±è´¥: {e}", file=sys.stderr)
    finally:
        page.close()
    
    return result


# ========== è¾“å‡ºæ ¼å¼åŒ– ==========

def print_results(data, as_json=False):
    """æ ¼å¼åŒ–æ‰“å°ç»“æœ"""
    if as_json:
        print(json.dumps(data, ensure_ascii=False, indent=2))
        return
    
    if isinstance(data, list):
        if not data:
            print("\nâŒ æœªæ‰¾åˆ°ç»“æœ")
            return
        print(f"\nâ”â”â” æ‰¾åˆ° {len(data)} æ¡ç»“æœ â”â”â”\n")
        for i, item in enumerate(data, 1):
            if "name" in item:
                # å¤§ä¼—ç‚¹è¯„æ ¼å¼
                print(f"[{i}] ğŸ“ {item['name']}")
                if item.get("rating"):
                    print(f"    â­ {item['rating']}")
                if item.get("price"):
                    print(f"    ğŸ’° {item['price']}")
                if item.get("category"):
                    print(f"    ğŸ·ï¸ {item['category']}")
                if item.get("address"):
                    print(f"    ğŸ“ {item['address']}")
                if item.get("url"):
                    href = item["url"]
                    if not href.startswith("http"):
                        href = "https://www.dianping.com" + href
                    print(f"    ğŸ”— {href}")
            elif "title" in item:
                # å°çº¢ä¹¦æ ¼å¼
                print(f"[{i}] ğŸ“ {item['title']}")
                if item.get("author"):
                    print(f"    ğŸ‘¤ {item['author']}")
                if item.get("likes"):
                    print(f"    â¤ï¸ {item['likes']}")
                if item.get("url"):
                    print(f"    ğŸ”— {item['url']}")
            elif "content" in item:
                # é€šç”¨æ ¼å¼
                print(f"[{i}] {item.get('content', '')[:200]}")
            print()
    
    elif isinstance(data, dict):
        if "data" in data:
            info = data["data"]
            print(f"\nâ”â”â” {data.get('title', '')} â”â”â”\n")
            for k, v in info.items():
                if isinstance(v, list):
                    print(f"  {k}:")
                    for item in v:
                        print(f"    - {item}")
                else:
                    print(f"  {k}: {v}")
        elif "error" in data:
            print(f"\nâŒ é”™è¯¯: {data['error']}")
        else:
            for k, v in data.items():
                print(f"  {k}: {v}")


# ========== ä¸»ç¨‹åº ==========

def main():
    parser = argparse.ArgumentParser(
        description="æµè§ˆå™¨æ•°æ®æŠ“å– â€” ç›´æ¥è®¿é—®å¤§ä¼—ç‚¹è¯„/å°çº¢ä¹¦ç­‰å¹³å°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æœç´¢å¤§ä¼—ç‚¹è¯„
  python3 browser_fetch.py --search "å¤–æ»©é¤å…" --site dianping

  # æœç´¢å°çº¢ä¹¦
  python3 browser_fetch.py --search "ä¸Šæµ·ç¾é£Ÿæ¨è" --site xiaohongshu

  # æŠ“å–å…·ä½“é¡µé¢
  python3 browser_fetch.py "https://www.dianping.com/shop/123456"

  # é¦–æ¬¡ä½¿ç”¨éœ€ç™»å½•ï¼ˆä¼šå¼¹å‡ºæµè§ˆå™¨çª—å£ï¼‰
  python3 browser_fetch.py --login dianping
        """
    )
    parser.add_argument("url", nargs="?", help="è¦æŠ“å–çš„ URL")
    parser.add_argument("--search", help="æœç´¢å…³é”®è¯")
    parser.add_argument("--site", choices=list(PLATFORMS.keys()), help="æœç´¢å¹³å°")
    parser.add_argument("--city", help="åŸå¸‚ï¼ˆä»…å¤§ä¼—ç‚¹è¯„ï¼Œå¦‚ï¼šä¸Šæµ·/åŒ—äº¬/å¹¿å·ï¼‰")
    parser.add_argument("--num", "-n", type=int, default=5, help="ç»“æœæ•°é‡ (é»˜è®¤ 5)")
    parser.add_argument("--json", action="store_true", help="JSON è¾“å‡º")
    parser.add_argument("--mobile", action="store_true", help="æ¨¡æ‹Ÿæ‰‹æœºè®¿é—®")
    parser.add_argument("--login", metavar="PLATFORM", choices=list(PLATFORMS.keys()),
                        help="ç™»å½•å¹³å°ä¿å­˜ Cookieï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # æ¨¡å¼0: ç™»å½•
    if args.login:
        login_platform(args.login)
        return
    
    if not args.url and not args.search:
        parser.print_help()
        return
    
    _ensure_playwright()
    
    # è‡ªåŠ¨æ£€æµ‹ CDP æ¨¡å¼
    cdp_available = _check_cdp_port() is not None
    if cdp_available:
        print("ğŸ”— æ£€æµ‹åˆ° Chrome Debug ç«¯å£ï¼Œä½¿ç”¨ CDP æ¨¡å¼", file=sys.stderr)
    
    with sync_playwright() as p:
        browser, ctx = create_browser_context(p, headless=True, mobile=args.mobile, use_cdp=cdp_available)
        
        # é CDP æ¨¡å¼ä¸‹åŠ è½½å·²ä¿å­˜çš„ Cookie
        if not cdp_available:
            if args.url:
                platform = detect_platform(args.url)
                if platform:
                    load_cookies(ctx, platform)
            elif args.site:
                load_cookies(ctx, args.site)
        
        try:
            if args.search and args.site:
                # æ¨¡å¼1: å¹³å°ç«™å†…æœç´¢
                results = search_platform(args.search, args.site, args.num, ctx, args.city)
                print_results(results, args.json)
                
            elif args.search:
                # æ²¡æŒ‡å®š siteï¼Œæç¤ºç”¨æˆ·
                print("è¯·æŒ‡å®šæœç´¢å¹³å°ï¼Œä¾‹å¦‚: --site dianping æˆ– --site xiaohongshu")
                parser.print_help()
                
            elif args.url:
                # æ¨¡å¼2: ç›´æ¥æŠ“å– URL
                result = fetch_url(args.url, ctx)
                print_results(result, args.json)
                
        finally:
            if not cdp_available:
                ctx.close()
            browser.close()


if __name__ == "__main__":
    main()
