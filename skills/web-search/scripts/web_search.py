#!/usr/bin/env python3
"""
ç½‘ç»œæœç´¢å·¥å…· â€” ä¸‰å¼•æ“è‡ªåŠ¨åˆ‡æ¢ï¼Œå®Œå…¨å…è´¹
åŸºäº GitHub å¤§ç¥æ–¹æ¡ˆï¼šGoogle HTML çˆ¬è™« + DuckDuckGo + Brave API

ç”¨æ³•:
  python3 web_search.py "æœç´¢å…³é”®è¯"
  python3 web_search.py "å…³é”®è¯" --engine google     # Googleï¼ˆæ¨èï¼Œè´¨é‡æœ€å¥½ï¼‰
  python3 web_search.py "å…³é”®è¯" --engine ddg         # DuckDuckGo
  python3 web_search.py "å…³é”®è¯" --engine brave       # Braveï¼ˆéœ€ API Keyï¼‰
"""

import argparse
import json
import os
import ssl
import sys
import time
import urllib.request
import urllib.parse
import re
import random


# ========== SSL ä¸Šä¸‹æ–‡ï¼ˆè§£å†³ macOS è‡ªç­¾åè¯ä¹¦é—®é¢˜ï¼‰==========
def _ssl_ctx():
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    return ctx


# ========== User-Agent æ± ï¼ˆé˜²åçˆ¬ï¼‰==========
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
]


def _ua():
    return random.choice(USER_AGENTS)


# ========== Google æœç´¢ï¼ˆå…è´¹çˆ¬è™«ï¼Œçµæ„Ÿæ¥è‡ª github.com/pskill9/web-searchï¼‰==========
def google_search(query, num_results=5):
    """Google HTML çˆ¬è™«æœç´¢ â€” å…è´¹ï¼Œè´¨é‡æœ€é«˜"""
    params = urllib.parse.urlencode({
        "q": query,
        "num": num_results + 2,  # å¤šè¯·æ±‚å‡ ä¸ªé˜²æ­¢è¿‡æ»¤
        "hl": "zh-CN",
        "gl": "cn"
    })
    url = f"https://www.google.com/search?{params}"

    req = urllib.request.Request(url, headers={
        "User-Agent": _ua(),
        "Accept": "text/html,application/xhtml+xml",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    })

    try:
        with urllib.request.urlopen(req, timeout=15, context=_ssl_ctx()) as resp:
            html = resp.read().decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"âš ï¸ Google æœç´¢å¤±è´¥: {e}", file=sys.stderr)
        return []

    results = []

    # æ–¹æ³•1: æå– <a href="/url?q=..." æ ¼å¼çš„é“¾æ¥
    pattern = r'<a[^>]+href="/url\?q=([^&"]+)[^"]*"[^>]*>(.*?)</a>'
    matches = re.findall(pattern, html, re.DOTALL)

    seen_urls = set()
    for raw_url, raw_title in matches:
        url = urllib.parse.unquote(raw_url)

        # è¿‡æ»¤ Google è‡ªèº«é“¾æ¥
        if any(skip in url for skip in ['google.com', 'youtube.com/results', 'accounts.google',
                                         'support.google', 'maps.google', 'translate.google']):
            continue
        if url in seen_urls:
            continue
        seen_urls.add(url)

        title = re.sub(r'<[^>]+>', '', raw_title).strip()
        if not title:
            continue

        results.append({
            "title": title,
            "url": url,
            "snippet": ""
        })

        if len(results) >= num_results:
            break

    # æå–æ‘˜è¦ï¼ˆsnippetsï¼‰
    snippet_patterns = [
        r'<span class="(?:st|aCOpRe)"[^>]*>(.*?)</span>',
        r'<div class="(?:BNeawe s3v9rd|IsZvec)"[^>]*>(.*?)</div>',
        r'data-sncf="[^"]*"[^>]*>(.*?)</(?:span|div)>',
    ]
    all_snippets = []
    for sp in snippet_patterns:
        found = re.findall(sp, html, re.DOTALL)
        all_snippets.extend(found)

    for i, r in enumerate(results):
        if i < len(all_snippets):
            r["snippet"] = re.sub(r'<[^>]+>', '', all_snippets[i]).strip()[:200]

    return results


# ========== DuckDuckGo æœç´¢ï¼ˆå…è´¹å¤‡é€‰ï¼‰==========
def ddg_search(query, num_results=5):
    """DuckDuckGo HTML æœç´¢ï¼ˆå®Œå…¨å…è´¹ï¼Œæ— éœ€ API Keyï¼‰"""
    url = "https://html.duckduckgo.com/html/"
    data = urllib.parse.urlencode({"q": query, "kl": "cn-zh"}).encode()

    req = urllib.request.Request(url, data=data, headers={
        "User-Agent": _ua(),
        "Content-Type": "application/x-www-form-urlencoded"
    })

    try:
        with urllib.request.urlopen(req, timeout=15, context=_ssl_ctx()) as resp:
            html = resp.read().decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"âš ï¸ DuckDuckGo æœç´¢å¤±è´¥: {e}", file=sys.stderr)
        return []

    results = []
    pattern = r'<a rel="nofollow" class="result__a" href="([^"]+)"[^>]*>(.*?)</a>'
    matches = re.findall(pattern, html)
    snippets = re.findall(r'<a class="result__snippet"[^>]*>(.*?)</a>', html, re.DOTALL)

    for i, (href, title) in enumerate(matches[:num_results]):
        actual_url = href
        uddg_match = re.search(r'uddg=([^&]+)', href)
        if uddg_match:
            actual_url = urllib.parse.unquote(uddg_match.group(1))
        clean_title = re.sub(r'<[^>]+>', '', title).strip()
        snippet = re.sub(r'<[^>]+>', '', snippets[i]).strip() if i < len(snippets) else ""
        results.append({"title": clean_title, "url": actual_url, "snippet": snippet})

    return results


# ========== Brave Search APIï¼ˆéœ€è¦ API Keyï¼Œä½†è´¨é‡å¥½ï¼‰==========
def brave_search(query, api_key, num_results=5):
    """Brave Search APIï¼ˆæ¯æœˆå…è´¹ $5 é¢åº¦ â‰ˆ 1000æ¬¡æœç´¢ï¼‰"""
    params = urllib.parse.urlencode({
        "q": query, "count": num_results,
        "search_lang": "zh-hans", "text_decorations": "false"
    })
    url = f"https://api.search.brave.com/res/v1/web/search?{params}"

    req = urllib.request.Request(url, headers={
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": api_key
    })

    try:
        with urllib.request.urlopen(req, timeout=15, context=_ssl_ctx()) as resp:
            raw = resp.read()
            if resp.headers.get('Content-Encoding') == 'gzip':
                import gzip
                raw = gzip.decompress(raw)
            data = json.loads(raw.decode("utf-8"))
    except Exception as e:
        print(f"âš ï¸ Brave æœç´¢å¤±è´¥: {e}", file=sys.stderr)
        return []

    results = []
    for item in data.get("web", {}).get("results", [])[:num_results]:
        results.append({
            "title": item.get("title", ""),
            "url": item.get("url", ""),
            "snippet": item.get("description", "")
        })
    return results

# ========== ç½‘é¡µæ­£æ–‡æŠ“å–ï¼ˆæœ¬åœ°ç‰ˆ Perplexity çš„æ ¸å¿ƒï¼‰==========
def fetch_page(url, max_chars=6000):
    """æŠ“å–ç½‘é¡µå¹¶æå–æ­£æ–‡ï¼ˆHTML â†’ çº¯æ–‡æœ¬ï¼‰"""
    req = urllib.request.Request(url, headers={
        "User-Agent": _ua(),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
        "DNT": "1",
    })

    try:
        with urllib.request.urlopen(req, timeout=15, context=_ssl_ctx()) as resp:
            content_type = resp.headers.get("Content-Type", "")
            if "text/html" not in content_type and "text/plain" not in content_type:
                return {"url": url, "error": f"éæ–‡æœ¬å†…å®¹: {content_type}"}

            # å¤„ç†ç¼–ç 
            charset = "utf-8"
            if "charset=" in content_type:
                charset = content_type.split("charset=")[-1].strip().split(";")[0]

            raw = resp.read()
            # å°è¯• gzip è§£å‹
            if resp.headers.get("Content-Encoding") == "gzip":
                import gzip
                raw = gzip.decompress(raw)

            html = raw.decode(charset, errors="ignore")
    except Exception as e:
        return {"url": url, "error": str(e)}

    # æå–æ ‡é¢˜
    title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.DOTALL | re.IGNORECASE)
    title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else ""

    # ç§»é™¤è„šæœ¬ã€æ ·å¼ã€å¯¼èˆªç­‰éæ­£æ–‡å†…å®¹
    for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript', 'iframe', 'svg']:
        html = re.sub(rf'<{tag}[^>]*>.*?</{tag}>', '', html, flags=re.DOTALL | re.IGNORECASE)

    # ç§»é™¤ HTML æ³¨é‡Š
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)

    # å°è¯•æå–æ­£æ–‡å®¹å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§é€‰æ‹©å™¨ï¼‰
    article = ""
    selectors = [
        r'<article[^>]*>(.*?)</article>',
        r'<main[^>]*>(.*?)</main>',
        r'<div[^>]*class="[^"]*(?:article|artibody|art_content|post_body|news_content|content_text|main-content|entry-content|post-content|article-body|article-content|news-body|detail-body|text-content|story-body|article-text|rich_media_content)[^"]*"[^>]*>(.*?)</div>',
        r'<div[^>]*id="[^"]*(?:article|artibody|content|main|post|entry|story)[^"]*"[^>]*>(.*?)</div>',
        r'<section[^>]*class="[^"]*(?:content|article|post|entry)[^"]*"[^>]*>(.*?)</section>',
    ]
    for selector in selectors:
        match = re.search(selector, html, re.DOTALL | re.IGNORECASE)
        if match and len(match.group(1)) > 200:  # è‡³å°‘ 200 å­—ç¬¦æ‰ç®—æœ‰æ•ˆ
            article = match.group(1)
            break

    # å¦‚æœæ²¡æ‰¾åˆ°æ–‡ç« å®¹å™¨ï¼Œç”¨æ•´ä¸ª body å†…çš„ <p> æ ‡ç­¾æ‹¼æ¥
    if not article:
        paragraphs = re.findall(r'<p[^>]*>(.*?)</p>', html, re.DOTALL | re.IGNORECASE)
        if paragraphs:
            article = '\n\n'.join(paragraphs)
        else:
            body_match = re.search(r'<body[^>]*>(.*?)</body>', html, re.DOTALL | re.IGNORECASE)
            article = body_match.group(1) if body_match else html

    # è½¬ä¸ºçº¯æ–‡æœ¬
    # æ®µè½å’Œæ¢è¡Œ
    text = re.sub(r'<br\s*/?\s*>', '\n', article)
    text = re.sub(r'</p>', '\n\n', text)
    text = re.sub(r'</div>', '\n', text)
    text = re.sub(r'</(?:h[1-6]|li|tr)>', '\n', text)
    # ç§»é™¤æ‰€æœ‰ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # è§£ç  HTML å®ä½“
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<')
    text = text.replace('&gt;', '>').replace('&quot;', '"').replace('&#39;', "'")
    text = re.sub(r'&#(\d+);', lambda m: chr(int(m.group(1))), text)
    # æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = text.strip()

    # æˆªæ–­åˆ°åˆç†é•¿åº¦
    if len(text) > max_chars:
        text = text[:max_chars] + f"\n\n[... å…¨æ–‡å·²æˆªæ–­ï¼Œå…± {len(text)} å­—ç¬¦ ...]"

    return {
        "url": url,
        "title": title,
        "content": text,
        "length": len(text)
    }


def deep_search(query, num_results=5, fetch_top=3, engine="auto"):
    """æ·±åº¦æœç´¢ï¼šæœç´¢ + è‡ªåŠ¨æŠ“å–å‰ N ç¯‡ç½‘é¡µå…¨æ–‡"""
    brave_key = os.environ.get("BRAVE_API_KEY", "")

    # 1. å…ˆæœç´¢
    if engine == "auto":
        engine = "brave" if brave_key else "google"

    results = []
    if engine == "brave" and brave_key:
        results = brave_search(query, brave_key, num_results)
    if engine == "google" or not results:
        results = google_search(query, num_results)
    if not results:
        results = ddg_search(query, num_results)

    if not results:
        return {"query": query, "results": [], "pages": []}

    # 2. æŠ“å–å‰ N ä¸ªç»“æœçš„ç½‘é¡µå…¨æ–‡
    pages = []
    for r in results[:fetch_top]:
        url = r.get("url", "")
        if not url or not url.startswith("http"):
            continue

        print(f"ğŸ“– æ­£åœ¨è¯»å–: {r.get('title', url)[:50]}...", file=sys.stderr)
        page = fetch_page(url)
        if "error" not in page:
            pages.append(page)

    return {"query": query, "results": results, "pages": pages}


# ========== ä¸»ç¨‹åº ==========
def main():
    parser = argparse.ArgumentParser(description="ç½‘ç»œæœç´¢ï¼ˆGoogle/DuckDuckGo/Brave ä¸‰å¼•æ“ + ç½‘é¡µæŠ“å–ï¼‰")
    parser.add_argument("query", nargs="?", help="æœç´¢å…³é”®è¯")
    parser.add_argument("--num", "-n", type=int, default=5, help="ç»“æœæ•°é‡ (é»˜è®¤: 5)")
    parser.add_argument("--engine", "-e", choices=["auto", "google", "ddg", "brave"], default="auto",
                        help="æœç´¢å¼•æ“ (é»˜è®¤: auto â†’ Google â†’ DDG è‡ªåŠ¨é™çº§)")
    parser.add_argument("--json", action="store_true", help="è¾“å‡º JSON æ ¼å¼")

    # ç½‘é¡µæŠ“å–æ¨¡å¼
    parser.add_argument("--fetch", metavar="URL", help="æŠ“å–æŒ‡å®š URL çš„ç½‘é¡µæ­£æ–‡")
    parser.add_argument("--max-chars", type=int, default=6000, help="æŠ“å–æœ€å¤§å­—ç¬¦æ•° (é»˜è®¤: 6000)")

    # æ·±åº¦æœç´¢æ¨¡å¼ï¼ˆ= æœç´¢ + æŠ“å–ï¼Œæœ¬åœ°ç‰ˆ Perplexityï¼‰
    parser.add_argument("--deep", action="store_true",
                        help="æ·±åº¦æœç´¢ï¼šæœç´¢åè‡ªåŠ¨æŠ“å–å‰ 3 ç¯‡ç½‘é¡µå…¨æ–‡ï¼ˆæœ¬åœ°ç‰ˆ Perplexityï¼‰")
    parser.add_argument("--fetch-top", type=int, default=3, help="æ·±åº¦æœç´¢æ—¶æŠ“å–å‰ N ç¯‡ (é»˜è®¤: 3)")

    args = parser.parse_args()
    brave_key = os.environ.get("BRAVE_API_KEY", "")

    # ===== æ¨¡å¼ 1: ç½‘é¡µæŠ“å– =====
    if args.fetch:
        result = fetch_page(args.fetch, args.max_chars)
        if args.json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            if "error" in result:
                print(f"âŒ æŠ“å–å¤±è´¥: {result['error']}")
            else:
                print(f"ğŸ“„ {result['title']}")
                print(f"ğŸ”— {result['url']}")
                print(f"ğŸ“ {result['length']} å­—ç¬¦")
                print(f"{'='*60}")
                print(result['content'])
        return

    # ===== æ¨¡å¼ 2: æ·±åº¦æœç´¢ï¼ˆæœ¬åœ°ç‰ˆ Perplexityï¼‰=====
    if args.deep and args.query:
        print(f"ğŸ”¬ æ·±åº¦æœç´¢: {args.query}\n", file=sys.stderr)
        result = deep_search(args.query, args.num, args.fetch_top, args.engine)

        if args.json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            print(f"ğŸ”¬ æ·±åº¦æœç´¢: {args.query}")
            print(f"ğŸ“Š æ‰¾åˆ° {len(result['results'])} ä¸ªç»“æœï¼Œå·²è¯»å– {len(result['pages'])} ç¯‡å…¨æ–‡\n")

            # å…ˆæ˜¾ç¤ºæœç´¢ç»“æœåˆ—è¡¨
            print("â”â”â” æœç´¢ç»“æœ â”â”â”")
            for i, r in enumerate(result['results'], 1):
                print(f"[{i}] {r['title']}")
                print(f"    ğŸ”— {r['url']}")
                if r.get('snippet'):
                    print(f"    ğŸ“ {r['snippet'][:120]}")
                print()

            # å†æ˜¾ç¤ºæŠ“å–çš„å…¨æ–‡
            for i, page in enumerate(result['pages'], 1):
                print(f"â”â”â” å…¨æ–‡ [{i}] {page.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]} â”â”â”")
                print(f"ğŸ”— {page['url']}")
                print(f"ğŸ“ {page['length']} å­—ç¬¦\n")
                print(page['content'][:3000])
                if page['length'] > 3000:
                    print(f"\n[... ä½™ä¸‹ {page['length'] - 3000} å­—ç¬¦å·²çœç•¥ ...]")
                print()
        return

    # ===== æ¨¡å¼ 3: æ™®é€šæœç´¢ =====
    if not args.query:
        parser.print_help()
        return

    # è‡ªåŠ¨é€‰æ‹©å¼•æ“ä¼˜å…ˆçº§: Brave(æœ‰Key) > Google > DuckDuckGo
    if args.engine == "auto":
        if brave_key:
            engine = "brave"
        else:
            engine = "google"
    else:
        engine = args.engine

    # æ‰§è¡Œæœç´¢ï¼ˆå¸¦è‡ªåŠ¨é™çº§ï¼‰
    results = []
    engine_used = engine

    if engine == "brave":
        if not brave_key:
            print("âš ï¸ æœªè®¾ç½® BRAVE_API_KEYï¼Œåˆ‡æ¢åˆ° Google", file=sys.stderr)
            engine = "google"
        else:
            results = brave_search(args.query, brave_key, args.num)

    if engine == "google":
        results = google_search(args.query, args.num)
        engine_used = "Google"
        if not results:
            print("âš ï¸ Google æ— ç»“æœï¼Œé™çº§åˆ° DuckDuckGo", file=sys.stderr)
            results = ddg_search(args.query, args.num)
            engine_used = "DuckDuckGo"

    if engine == "ddg":
        results = ddg_search(args.query, args.num)
        engine_used = "DuckDuckGo"

    # è¾“å‡º
    if args.json:
        print(json.dumps({"engine": engine_used, "results": results}, ensure_ascii=False, indent=2))
    else:
        print(f"ğŸ” [{engine_used}] æœç´¢: {args.query}\n")
        if not results:
            print("æœªæ‰¾åˆ°ç»“æœã€‚")
            return
        for i, r in enumerate(results, 1):
            print(f"[{i}] {r['title']}")
            print(f"    ğŸ”— {r['url']}")
            if r['snippet']:
                print(f"    ğŸ“ {r['snippet'][:150]}")
            print()


if __name__ == "__main__":
    main()

